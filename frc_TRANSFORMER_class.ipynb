{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import AutoTokenizer\n",
    "import natasha\n",
    "import mlflow\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import psycopg2\n",
    "from torchinfo import summary\n",
    "from datetime import datetime\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "DB_HOST = os.environ.get(\"DB_HOST\")\n",
    "DB_PORT = os.environ.get(\"DB_PORT\")\n",
    "DB_USER = os.environ.get(\"DB_USER\")\n",
    "DB_PASS = os.environ.get(\"DB_PASS\")\n",
    "DB_NAME = os.environ.get(\"DB_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.strptime(\"2024-01-09 00:00:00\", \"%Y-%d-%m %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_config, device, train_loader, test_loader,\n",
    "                model = None, optimizer = None, scheduler = None,\n",
    "                weight_decay = 1e-3,\n",
    "                lr = 1e-3,\n",
    "                start_epoch = 0,\n",
    "                epochs = 10):\n",
    "    try:\n",
    "        best_accuracy = 0\n",
    "        if model == None:\n",
    "            model = Forecaster(model_config).to(device)\n",
    "        if optimizer == None:\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        if scheduler is not None:\n",
    "            scheduler = StepLR(optimizer, step_size=scheduler[\"step_size\"], gamma=scheduler[\"gamma\"])\n",
    "        with open(\"artifacts/model_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(summary(model)))\n",
    "        mlflow.log_artifacts(\"artifacts\")\n",
    "        BCE = nn.BCELoss()\n",
    "        for ep in tqdm(range(start_epoch, start_epoch + epochs)):\n",
    "            epoch_loss_train = 0; epoch_loss_test = 0\n",
    "            accuracy_train = 0; accuracy_test = 0\n",
    "            # TRAIN\n",
    "            for sample in tqdm(train_loader, desc = f\"Epoch: {ep}\"):\n",
    "                X_time_ = sample[0].to(device)\n",
    "                Add_feature_ts_ = sample[1].to(device)\n",
    "                X_news_embedd_ = sample[2].to(device)\n",
    "                X_news_mask_ = sample[3].to(device)\n",
    "                TICK_ = sample[4].to(device)\n",
    "                Y_ = sample[5].to(device)\n",
    "                pred = model(\n",
    "                    time = X_time_,\n",
    "                    add_time = Add_feature_ts_,\n",
    "                    ticker = TICK_,\n",
    "                    news = X_news_embedd_,\n",
    "                    news_mask = X_news_mask_\n",
    "                )\n",
    "                loss = BCE(pred, Y_.type(torch.float))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                accuracy_train += pred.round().int().eq(Y_).sum().item(); epoch_loss_train += loss * len(pred)\n",
    "            # TEST\n",
    "            for sample in tqdm(test_loader, desc = f\"Epoch: {ep}\"):\n",
    "                X_time_ = sample[0].to(device)\n",
    "                Add_feature_ts_ = sample[1].to(device)\n",
    "                X_news_embedd_ = sample[2].to(device)\n",
    "                X_news_mask_ = sample[3].to(device)\n",
    "                TICK_ = sample[4].to(device)\n",
    "                Y_ = sample[5].to(device)\n",
    "                with torch.no_grad():\n",
    "                    pred = model(\n",
    "                        time = X_time_,\n",
    "                        add_time = Add_feature_ts_,\n",
    "                        ticker = TICK_,\n",
    "                        news = X_news_embedd_,\n",
    "                        news_mask = X_news_mask_\n",
    "                    )\n",
    "                loss = BCE(pred, Y_.type(torch.float))\n",
    "                accuracy_test += pred.round().int().eq(Y_).sum().item(); epoch_loss_test += loss * len(pred)\n",
    "            accuracy_train /= len(train_loader.dataset); accuracy_test /= len(test_loader.dataset)\n",
    "    \n",
    "            mlflow.log_metrics({\"train loss\":epoch_loss_train, \"test loss\":epoch_loss_test, \"train_acc\":accuracy_train, \"test_acc\":accuracy_test}, step=ep)\n",
    "            if accuracy_test > best_accuracy and accuracy_train * 0.85 <= accuracy_test:\n",
    "                mlflow.pytorch.log_model(model, f\"epoch_{ep}\")\n",
    "                best_accuracy = accuracy_test\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            if ep % 10 == 0:\n",
    "                clear_output()\n",
    "        print(start_epoch, epochs, epoch_loss_test, epoch_loss_train)\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(model, \"model.pt\")\n",
    "        torch.save(optimizer.state_dict(), \"optimizer_state_dict.pt\")\n",
    "        print(\"save with error\")\n",
    "        return model, optimizer\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        torch.save(model, \"model.pt\")\n",
    "        torch.save(optimizer.state_dict(), \"optimizer_state_dict.pt\")\n",
    "        print(\"done\")\n",
    "        return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_accuracy(pred:torch.Tensor, true:torch.Tensor):\n",
    "    return true.argmax(dim=1).eq(pred.argmax(dim=1)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_clean(text: str, segmenter:natasha.Segmenter, word: str = None):\n",
    "    sentenses = []\n",
    "    doc = natasha.Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    for sent in doc.sents:\n",
    "        if word.lower() in sent.text.lower():\n",
    "           sentenses.append(sent.text) \n",
    "    sentenses = ' '.join(sentenses)\n",
    "    return sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_np_time_seties(df_pd:pd.DataFrame, index_column_time=None, x_step=50, y_step=10, columns_x=[1, 2, 3, 4, 5], columns_y=[2, 3]) -> np.array:   \n",
    "    df_pd = df_pd.reset_index(drop=True)\n",
    "    if index_column_time != None:\n",
    "       time = df_pd.loc[:, index_column_time].copy()\n",
    "    \n",
    "    num_samples = df_pd.shape[0]\n",
    "    num_samples = np.floor(num_samples / (x_step+y_step)).astype(int)\n",
    "    y_features = len(columns_y)\n",
    "    x_features = len(columns_x)\n",
    "\n",
    "    \n",
    "\n",
    "    X, Y = np.empty((num_samples, x_step, x_features)), np.empty((num_samples, y_step, y_features))\n",
    "    if index_column_time != None:\n",
    "        T = np.empty((num_samples, 2), dtype=pd.Timestamp)\n",
    "\n",
    "    start_x = 0\n",
    "    start_y = x_step\n",
    "    for index in tqdm(range(0, num_samples)):\n",
    "        if df_pd.loc[start_x:start_x + x_step - 1, columns_x].shape[0] == 0:\n",
    "            print(df_pd.loc[start_x-60:start_x + x_step - 1, columns_x])\n",
    "            print('start, end', start_x, start_y)\n",
    "        X[index] = df_pd.loc[start_x:start_x + x_step - 1, columns_x].to_numpy().reshape((x_step, x_features))\n",
    "        \n",
    "        Y[index] = df_pd.loc[start_y:start_y + y_step - 1, columns_y].to_numpy().reshape((y_step, y_features))\n",
    "        \n",
    "        if index_column_time != None:\n",
    "            T[index] = [time.loc[start_x:start_x + x_step].min(), time.loc[start_x:start_x + x_step].max()]\n",
    "\n",
    "        start_x = start_x + x_step + y_step\n",
    "        start_y = start_y + x_step + y_step\n",
    "        \n",
    "    if index_column_time != None:\n",
    "        return X, Y, T\n",
    "    else:\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Заменяем управляющие символы (\\n, \\t и прочие) на пробел\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Удаляем ссылки (http, https, www)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+|www\\.\\S+|https?:\\S+', '', text)\n",
    "\n",
    "    # Удаляем смайлики (оставляем только буквы, цифры и знаки препинания)\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\\"\\'():;\\-]', '', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsi(prices: torch.Tensor, period: int = 14):\n",
    "    \"\"\"\n",
    "    Вычисляет RSI для каждого временного ряда из батча.\n",
    "    prices: тензор размера [batch, seq_len]\n",
    "    Возвращает тензор RSI для последнего временного шага: [batch]\n",
    "    \"\"\"\n",
    "    # Вычисляем изменения цены\n",
    "    delta = prices[:, 1:] - prices[:, :-1]  # [batch, seq_len-1]\n",
    "    # Вычисляем прирост и потери\n",
    "    gain = torch.clamp(delta, min=0)\n",
    "    loss = torch.clamp(-delta, min=0)\n",
    "    \n",
    "    # Если шагов меньше, чем период, возвращаем NaN\n",
    "    if delta.shape[1] < period:\n",
    "        raise ValueError(\"Длина временного ряда меньше, чем период RSI\")\n",
    "    \n",
    "    # Усредняем приросты и потери по последнему периоду\n",
    "    avg_gain = gain[:, -period:].mean(dim=1)\n",
    "    avg_loss = loss[:, -period:].mean(dim=1)\n",
    "    \n",
    "    rs = avg_gain / (avg_loss + 1e-8)  # добавляем маленькое число, чтобы не делить на 0\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi  # [batch]\n",
    "\n",
    "def compute_ema(prices: torch.Tensor, period: int):\n",
    "    \"\"\"\n",
    "    Вычисляет EMA по заданному периоду для каждого временного ряда.\n",
    "    prices: тензор размера [batch, seq_len]\n",
    "    Возвращает EMA с той же размерностью [batch, seq_len].\n",
    "    \"\"\"\n",
    "    alpha = 2 / (period + 1)\n",
    "    ema = torch.zeros_like(prices)\n",
    "    ema[:, 0] = prices[:, 0]  # инициализация первым значением\n",
    "    # Итеративное вычисление EMA\n",
    "    for t in range(1, prices.shape[1]):\n",
    "        ema[:, t] = alpha * prices[:, t] + (1 - alpha) * ema[:, t-1]\n",
    "    return ema\n",
    "\n",
    "def compute_macd(prices: torch.Tensor, short_period: int = 5, long_period: int = 12, signal_period: int = 5):\n",
    "    \"\"\"\n",
    "    Вычисляет MACD, сигнальную линию и гистограмму для каждого временного ряда.\n",
    "    Из-за малого числа шагов в ряду (24 шага) стандартные периоды (12, 26, 9) могут быть не применимы.\n",
    "    Поэтому можно использовать меньшие периоды.\n",
    "    \"\"\"\n",
    "    ema_short = compute_ema(prices, short_period)\n",
    "    ema_long  = compute_ema(prices, long_period)\n",
    "    \n",
    "    # MACD как разница EMA короткого и длинного периодов\n",
    "    macd = ema_short - ema_long\n",
    "    \n",
    "    # Сигнальная линия – EMA от MACD\n",
    "    signal_line = compute_ema(macd, signal_period)\n",
    "    # Гистограмма – разница между MACD и сигнальной линией\n",
    "    histogram = macd - signal_line\n",
    "    return macd, signal_line, histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_list_float(row):\n",
    "    embedding = [float(value) for value in row[\"embedding\"][1:-1].split(\",\")]\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"matvej-melikhov/ruBERT-finetuned-lenta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"SELECT news.text, news.ticker, dsc.name, dsc.sector, news.date, emb.embedding FROM\n",
    "            news_with_ticker news LEFT JOIN ticker_describe dsc ON news.ticker = dsc.ticker\n",
    "            LEFT JOIN news_with_ticker_embedd emb on news.id = emb.id\n",
    "            \"\"\"\n",
    "\n",
    "with psycopg2.connect(dbname = DB_NAME, user = DB_USER, password = DB_PASS, host = DB_HOST, port = DB_PORT) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query)\n",
    "        news = pd.DataFrame(cur.fetchall(), columns=[\"text\", \"ticker\", \"name\", \"sector\", \"date\", \"embedding\"])\n",
    "\n",
    "# query = f\"\"\"SELECT news.date, news.text, news.channel, emb.embedding FROM\n",
    "#             news LEFT JOIN news_embedd emb ON news.id = emb.id\n",
    "#             \"\"\"\n",
    "\n",
    "# with psycopg2.connect(dbname = DB_NAME, user = DB_USER, password = DB_PASS, host = DB_HOST, port = DB_PORT) as conn:\n",
    "#     with conn.cursor() as cur:\n",
    "#         cur.execute(query)\n",
    "#         news = pd.DataFrame(cur.fetchall(), columns=[\"date\", \"text\", \"channel\", \"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[\"embedding\"] = news.apply(text_to_list_float, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"SELECT * FROM price_hour\"\"\"\n",
    "with psycopg2.connect(dbname = DB_NAME, user = DB_USER, password = DB_PASS, host = DB_HOST, port = DB_PORT) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query)\n",
    "        time_series = pd.DataFrame(cur.fetchall(), columns=[\"open\", \"low\", \"high\", \"close\", \"volume\", \"date\", \"ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optional\n",
    "time_series = time_series[time_series[\"date\"] <= current_date]\n",
    "news = news[news[\"date\"] <= current_date]\n",
    "tickers_ = np.array(['MGNT', 'PHOR', 'MAGN', 'VTBR', 'RUAL', 'ALRS', 'FLOT', 'TATN',\n",
    "       'RTKM', 'MTSS', 'NVTK', 'SNGS', 'ROSN', 'CHMF', 'KMAZ', 'YDEX',\n",
    "       'AFLT', 'SBER', 'MVID', 'MTLR', 'GAZP', 'LENT', 'BSPB', 'SIBN',\n",
    "       'HYDR', 'NLMK', 'RASP', 'GMKN', 'AFKS', 'PLZL', 'LKOH'],\n",
    "      dtype=object)\n",
    "\n",
    "\n",
    "time_series = time_series[time_series[\"ticker\"].isin(tickers_)]\n",
    "if news.get(\"ticker\") is not None:\n",
    "    news = news[news[\"ticker\"].isin(tickers_)]\n",
    "time_series.reset_index(inplace=True, drop=True)\n",
    "news.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5601.0</td>\n",
       "      <td>5601.0</td>\n",
       "      <td>5601.0</td>\n",
       "      <td>5601.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-08-30 09:00:00</td>\n",
       "      <td>MGNT</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5601.0</td>\n",
       "      <td>5578.5</td>\n",
       "      <td>5623.5</td>\n",
       "      <td>5590.0</td>\n",
       "      <td>38026</td>\n",
       "      <td>2021-08-30 10:00:00</td>\n",
       "      <td>MGNT</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5592.0</td>\n",
       "      <td>5558.0</td>\n",
       "      <td>5593.5</td>\n",
       "      <td>5577.5</td>\n",
       "      <td>26215</td>\n",
       "      <td>2021-08-30 11:00:00</td>\n",
       "      <td>MGNT</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5577.5</td>\n",
       "      <td>5557.5</td>\n",
       "      <td>5577.5</td>\n",
       "      <td>5561.5</td>\n",
       "      <td>14164</td>\n",
       "      <td>2021-08-30 12:00:00</td>\n",
       "      <td>MGNT</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5561.5</td>\n",
       "      <td>5544.0</td>\n",
       "      <td>5579.0</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>18672</td>\n",
       "      <td>2021-08-30 13:00:00</td>\n",
       "      <td>MGNT</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380727</th>\n",
       "      <td>6131.0</td>\n",
       "      <td>6116.0</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>400</td>\n",
       "      <td>2024-08-31 19:00:00</td>\n",
       "      <td>LKOH</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>-9.659258e-01</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380728</th>\n",
       "      <td>6132.0</td>\n",
       "      <td>6124.0</td>\n",
       "      <td>6139.5</td>\n",
       "      <td>6133.0</td>\n",
       "      <td>281</td>\n",
       "      <td>2024-08-31 20:00:00</td>\n",
       "      <td>LKOH</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380729</th>\n",
       "      <td>6133.0</td>\n",
       "      <td>6112.5</td>\n",
       "      <td>6144.0</td>\n",
       "      <td>6120.0</td>\n",
       "      <td>719</td>\n",
       "      <td>2024-08-31 21:00:00</td>\n",
       "      <td>LKOH</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380730</th>\n",
       "      <td>6120.0</td>\n",
       "      <td>6119.0</td>\n",
       "      <td>6133.5</td>\n",
       "      <td>6122.5</td>\n",
       "      <td>245</td>\n",
       "      <td>2024-08-31 22:00:00</td>\n",
       "      <td>LKOH</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380731</th>\n",
       "      <td>6130.0</td>\n",
       "      <td>6115.0</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>6129.0</td>\n",
       "      <td>166</td>\n",
       "      <td>2024-08-31 23:00:00</td>\n",
       "      <td>LKOH</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380732 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          open     low    high   close  volume                date ticker  \\\n",
       "0       5601.0  5601.0  5601.0  5601.0      13 2021-08-30 09:00:00   MGNT   \n",
       "1       5601.0  5578.5  5623.5  5590.0   38026 2021-08-30 10:00:00   MGNT   \n",
       "2       5592.0  5558.0  5593.5  5577.5   26215 2021-08-30 11:00:00   MGNT   \n",
       "3       5577.5  5557.5  5577.5  5561.5   14164 2021-08-30 12:00:00   MGNT   \n",
       "4       5561.5  5544.0  5579.0  5560.0   18672 2021-08-30 13:00:00   MGNT   \n",
       "...        ...     ...     ...     ...     ...                 ...    ...   \n",
       "380727  6131.0  6116.0  6132.0  6132.0     400 2024-08-31 19:00:00   LKOH   \n",
       "380728  6132.0  6124.0  6139.5  6133.0     281 2024-08-31 20:00:00   LKOH   \n",
       "380729  6133.0  6112.5  6144.0  6120.0     719 2024-08-31 21:00:00   LKOH   \n",
       "380730  6120.0  6119.0  6133.5  6122.5     245 2024-08-31 22:00:00   LKOH   \n",
       "380731  6130.0  6115.0  6132.0  6129.0     166 2024-08-31 23:00:00   LKOH   \n",
       "\n",
       "        weekday  hour      hour_sin  hour_cos  weekday_sin  weekday_cos  \n",
       "0             1     9  7.071068e-01 -0.707107     0.781831      0.62349  \n",
       "1             1    10  5.000000e-01 -0.866025     0.781831      0.62349  \n",
       "2             1    11  2.588190e-01 -0.965926     0.781831      0.62349  \n",
       "3             1    12  1.224647e-16 -1.000000     0.781831      0.62349  \n",
       "4             1    13 -2.588190e-01 -0.965926     0.781831      0.62349  \n",
       "...         ...   ...           ...       ...          ...          ...  \n",
       "380727        6    19 -9.659258e-01  0.258819    -0.781831      0.62349  \n",
       "380728        6    20 -8.660254e-01  0.500000    -0.781831      0.62349  \n",
       "380729        6    21 -7.071068e-01  0.707107    -0.781831      0.62349  \n",
       "380730        6    22 -5.000000e-01  0.866025    -0.781831      0.62349  \n",
       "380731        6    23 -2.588190e-01  0.965926    -0.781831      0.62349  \n",
       "\n",
       "[380732 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series['weekday'] = time_series['date'].apply(lambda row: row.weekday()) + 1\n",
    "time_series['hour'] = time_series['date'].apply(lambda row: row.hour)\n",
    "time_series['hour_sin'] = np.sin(np.pi * 2 * time_series['hour'] / 24)\n",
    "time_series['hour_cos'] = np.cos(np.pi * 2 * time_series['hour'] / 24)\n",
    "time_series['weekday_sin'] = np.sin(np.pi * 2 * time_series['weekday'] / 7)\n",
    "time_series['weekday_cos'] = np.cos(np.pi * 2 * time_series['weekday'] / 7)\n",
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_step = 24\n",
    "y_step = 12\n",
    "x_column = ['open', 'low', 'high', 'close', 'volume', 'weekday_sin', 'weekday_cos', 'hour_sin', 'hour_cos']\n",
    "y_column = ['close']\n",
    "x_feature = len(x_column)\n",
    "y_feature = len(y_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_batch(time_series):\n",
    "    X_time = np.zeros((0, x_step, x_feature))\n",
    "    Y = np.zeros((0, y_step, y_feature))\n",
    "    T = np.zeros((0, 2))\n",
    "    TICK = np.empty((0))\n",
    "\n",
    "    for ticker in time_series['ticker'].unique():\n",
    "        X_time_, Y_, T_ = pd_to_np_time_seties(time_series[time_series['ticker'] == ticker], \n",
    "                                            index_column_time='date', \n",
    "                                            x_step=x_step, \n",
    "                                            y_step=y_step, \n",
    "                                            columns_x=x_column, \n",
    "                                            columns_y=y_column)\n",
    "        \n",
    "        X_time = np.concatenate((X_time, X_time_), axis=0)\n",
    "        Y = np.concatenate((Y, Y_), axis=0)\n",
    "        T = np.concatenate((T, T_), axis=0)\n",
    "        TICK = np.concatenate((TICK, np.array([ticker] * X_time_.shape[0])), axis=0)\n",
    "\n",
    "    X_time = torch.from_numpy(X_time).float()\n",
    "    Y = torch.from_numpy(Y).float()\n",
    "    if Y.shape[-1] == 1:\n",
    "        Y = Y.reshape([Y.shape[0], -1])\n",
    "\n",
    "    VaR = (Y[:, 1:] - Y[:, :-1]) / Y[:, :-1]\n",
    "    VaR_95 = torch.quantile(VaR, 0.05, 1)\n",
    "    VaR_95 = VaR_95.reshape([-1, 1])\n",
    "\n",
    "    Y = ((Y - X_time[:, -1:, 3]) / X_time[:, -1:, 3]) > 0.01\n",
    "    Y = Y.sum(dim=1)\n",
    "    Y[Y !=0 ] = 1\n",
    "\n",
    "    X_time[:, :, :5] = torch.log10(X_time[:, :, :5])\n",
    "    X_time[X_time == -torch.inf] = 0\n",
    "\n",
    "    Add_feature_ts = torch.empty([X_time.shape[0], 5])\n",
    "    # rsi\n",
    "    Add_feature_ts[:, 0] = compute_rsi(X_time[:, :, 3])\n",
    "    # mean\n",
    "    Add_feature_ts[:, 1] = X_time[:, :, 3].mean(dim=1)\n",
    "    Add_feature_ts[:, 2] = X_time[:, :, 4].mean(dim=1)\n",
    "    # std\n",
    "    Add_feature_ts[:, 3] = X_time[:, :, 3].std(dim=1)\n",
    "    Add_feature_ts[:, 4] = X_time[:, :, 4].std(dim=1)\n",
    "\n",
    "    # macd, signal_line, histogram\n",
    "    macd, signal_line, histogram = compute_macd(X_time[:, :, 3], 3, 5, 4)\n",
    "    X_time = torch.cat([X_time, macd.unsqueeze(-1), signal_line.unsqueeze(-1), histogram.unsqueeze(-1)], dim=2)\n",
    "    \n",
    "    return X_time, Add_feature_ts, T, TICK, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_date = np.sort(time_series[\"date\"].unique())\n",
    "sep_index = int(len(unique_date) * 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:00<00:00, 787.17it/s]\n",
      "100%|██████████| 308/308 [00:00<00:00, 794.47it/s]\n",
      "100%|██████████| 309/309 [00:00<00:00, 832.53it/s]\n",
      "100%|██████████| 298/298 [00:00<00:00, 817.34it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 818.56it/s]\n",
      "100%|██████████| 309/309 [00:00<00:00, 780.30it/s]\n",
      "100%|██████████| 235/235 [00:00<00:00, 830.06it/s]\n",
      "100%|██████████| 304/304 [00:00<00:00, 794.29it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 815.40it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 826.04it/s]\n",
      "100%|██████████| 308/308 [00:00<00:00, 808.20it/s]\n",
      "100%|██████████| 303/303 [00:00<00:00, 846.33it/s]\n",
      "100%|██████████| 309/309 [00:00<00:00, 851.74it/s]\n",
      "100%|██████████| 310/310 [00:00<00:00, 846.94it/s]\n",
      "100%|██████████| 221/221 [00:00<00:00, 898.32it/s]\n",
      "100%|██████████| 310/310 [00:00<00:00, 820.56it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 833.89it/s]\n",
      "100%|██████████| 310/310 [00:00<00:00, 829.18it/s]\n",
      "100%|██████████| 264/264 [00:00<00:00, 840.41it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 858.30it/s]\n",
      "100%|██████████| 310/310 [00:00<00:00, 841.79it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 920.27it/s]\n",
      "100%|██████████| 220/220 [00:00<00:00, 907.35it/s]\n",
      "100%|██████████| 267/267 [00:00<00:00, 839.82it/s]\n",
      "100%|██████████| 297/297 [00:00<00:00, 844.39it/s]\n",
      "100%|██████████| 309/309 [00:00<00:00, 839.84it/s]\n",
      "100%|██████████| 228/228 [00:00<00:00, 877.41it/s]\n",
      "100%|██████████| 305/305 [00:00<00:00, 849.05it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 875.83it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 844.16it/s]\n",
      "100%|██████████| 309/309 [00:00<00:00, 856.43it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1127.50it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1131.81it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1057.90it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 1133.61it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1177.84it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1185.15it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1177.80it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1200.02it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1177.76it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 999.98it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1127.64it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1204.61it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1173.94it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1200.01it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1127.62it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 1199.68it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1200.02it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1200.05it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1152.17it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 1181.87it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1209.13it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 1116.27it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 1181.83it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 1155.54it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1177.77it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1177.54it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 1181.81it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1200.04it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 1204.53it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 981.82it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 1136.30it/s]\n"
     ]
    }
   ],
   "source": [
    "X_time_train, Add_feature_ts_train, T_train, TICK_train, Y_train = return_batch(time_series[time_series[\"date\"].isin(unique_date[:sep_index])])\n",
    "X_time_test, Add_feature_ts_test, T_test, TICK_test, Y_test = return_batch(time_series[time_series[\"date\"].isin(unique_date[sep_index:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]), tensor([5899, 3017]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]), tensor([1225,  400]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance(Y):\n",
    "    indices_class_0 = torch.nonzero(Y == 0).squeeze()\n",
    "    indices_class_1 = torch.nonzero(Y == 1).squeeze()\n",
    "\n",
    "    class_sample = Y.unique(return_counts=True)[1].min().item()\n",
    "\n",
    "    torch.manual_seed(12312)\n",
    "    permuted_indices_class_0 = indices_class_0[torch.randperm(len(indices_class_0))][:class_sample]\n",
    "    torch.manual_seed(12312)\n",
    "    permuted_indices_class_1 = indices_class_1[torch.randperm(len(indices_class_1))][:class_sample]\n",
    "    \n",
    "    permuted_indices = torch.cat([permuted_indices_class_0, permuted_indices_class_1])\n",
    "    torch.manual_seed(12312)\n",
    "    permuted_indices = permuted_indices[torch.randperm(len(permuted_indices))]\n",
    "    return permuted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_balance = class_balance(Y_train)\n",
    "test_class_balance = class_balance(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_time_train, Add_feature_ts_train, T_train, TICK_train, Y_train = X_time_train[train_class_balance], Add_feature_ts_train[train_class_balance], T_train[train_class_balance], TICK_train[train_class_balance], Y_train[train_class_balance]\n",
    "X_time_test, Add_feature_ts_test, T_test, TICK_test, Y_test = X_time_test[test_class_balance], Add_feature_ts_test[test_class_balance], T_test[test_class_balance], TICK_test[test_class_balance], Y_test[test_class_balance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_news = np.zeros(T.shape[0])\n",
    "# for index in tqdm(range(T.shape[0])):\n",
    "#     max_news[index] = news[(news.date >= T[index][0]) & (news.date <= T[index][1]) & (news.ticker == TICK[index])].shape[0]\n",
    "\n",
    "# stat_table = pd.DataFrame({'stat':max_news, 'ticker':TICK})\n",
    "# stat_table = stat_table.groupby(by='ticker').agg(['min', 'max', 'mean', 'std']).stat.sort_values(by='mean', ascending=False)\n",
    "\n",
    "# stat_table.sort_values(by=\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_news = 100\n",
    "hidden_size = len(news['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(n_sample, T, TICK):\n",
    "    X_news_embedd = torch.full((n_sample, max_n_news, hidden_size), tokenizer.pad_token_id, dtype=torch.float32)\n",
    "    X_news_mask = torch.zeros((n_sample, max_n_news))\n",
    "    for index in tqdm(range(T.shape[0])):\n",
    "        if news.get(\"ticker\") is not None:\n",
    "            ids_mask = news[(news.date >= T[index][0]) & (news.date <= T[index][1]) & (news.ticker == TICK[index])]\n",
    "        else:\n",
    "            ids_mask = news[(news.date >= T[index][0]) & (news.date <= T[index][1])]\n",
    "        if ids_mask.shape[0] > max_n_news:\n",
    "            np.random.seed(12312)\n",
    "            ids_mask = ids_mask.sample(max_n_news)\n",
    "\n",
    "        embedding = ids_mask.embedding.tolist()\n",
    "\n",
    "        if embedding != []:\n",
    "            embedding = torch.stack([torch.tensor(e) for e in embedding])\n",
    "            X_news_embedd[index, :embedding.shape[0], :] = embedding\n",
    "            X_news_mask[index, :embedding.shape[0]] = torch.ones((embedding.shape[0]))\n",
    "\n",
    "    return X_news_embedd, X_news_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6034/6034 [00:19<00:00, 306.76it/s]\n",
      "100%|██████████| 800/800 [00:02<00:00, 323.48it/s]\n"
     ]
    }
   ],
   "source": [
    "X_news_embedd_train, X_news_mask_train = get_news(X_time_train.shape[0], T_train, TICK_train)\n",
    "X_news_embedd_test, X_news_mask_test = get_news(X_time_test.shape[0], T_test, TICK_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder().fit(tickers_)\n",
    "TICK_train = torch.from_numpy(label_encoder.fit_transform(TICK_train))\n",
    "TICK_test = torch.from_numpy(label_encoder.fit_transform(TICK_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_news_mask_test = X_news_mask_test.int()\n",
    "X_news_mask_train = X_news_mask_train.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LOADER = TensorDataset(X_time_train, Add_feature_ts_train, X_news_embedd_train, X_news_mask_train, TICK_train, Y_train)\n",
    "TEST_LOADER = TensorDataset(X_time_test, Add_feature_ts_test, X_news_embedd_test, X_news_mask_test, TICK_test, Y_test)\n",
    "\n",
    "batch_size = 50\n",
    "TRAIN_LOADER = DataLoader(TRAIN_LOADER, batch_size=50)\n",
    "TEST_LOADER = DataLoader(TEST_LOADER, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLOCK_TS(nn.Module):\n",
    "    def __init__(self, config_:dict = None):\n",
    "        super().__init__()\n",
    "        self.config = {\n",
    "            \"x_feature\" : 4,\n",
    "            \"hidden_size\" : 36,\n",
    "            \"num_layers\" : 4,\n",
    "            \"output_size\": 70\n",
    "        }\n",
    "\n",
    "        if config_ != None:\n",
    "            for key in config_.keys():\n",
    "                self.config[key] = config_[key]\n",
    "        self.lstm = nn.LSTM(self.config[\"x_feature\"], self.config[\"hidden_size\"], self.config[\"num_layers\"], batch_first=True)\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.LayerNorm(self.config[\"hidden_size\"]),\n",
    "        #     nn.Linear(self.config[\"hidden_size\"], self.config[\"hidden_size\"] * 4),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(self.config[\"hidden_size\"] * 4, self.config[\"output_size\"])\n",
    "        # )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.config[\"hidden_size\"], self.config[\"output_size\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, time:torch.Tensor):\n",
    "        h0 = torch.zeros(self.config[\"num_layers\"], time.size(0), self.config[\"hidden_size\"]).to(time.device)\n",
    "        c0 = torch.zeros(self.config[\"num_layers\"], time.size(0), self.config[\"hidden_size\"]).to(time.device)\n",
    "        out, _ = self.lstm(time, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size:int, num_heads:int):\n",
    "        super().__init__()\n",
    "        self.cross_attn = torch.nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        self.activate = nn.SiLU()\n",
    "    def forward(self, time_series, news):\n",
    "        time_series = time_series\n",
    "        attn_output, _ = self.cross_attn(time_series, news, news)\n",
    "        return self.activate(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forecaster(nn.Module):\n",
    "    def __init__(self, config_, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.config = {\n",
    "            \"attention_layer\" : 4,\n",
    "            \"hidden_size\" : 64,\n",
    "            \"num_heads\" : 4,\n",
    "            \"news_embedd\" : 768,\n",
    "            \"n_add_feature_ts\" : 5,\n",
    "            \"tick_count\" : 0,\n",
    "            \"ticker_embedd_dim\":0,\n",
    "            \"block_ts_config\" : {\n",
    "                \"x_feature\" : 4,\n",
    "                \"hidden_size\" : 36,\n",
    "                \"num_layers\" : 4,\n",
    "                \"output_size\":70\n",
    "            }\n",
    "        }\n",
    "        if config_ != None:\n",
    "            for key in config_.keys():\n",
    "                self.config[key] = config_[key]\n",
    "        self.config[\"block_ts_config\"][\"output_size\"] = self.config[\"hidden_size\"]\n",
    "        self.config[\"hidden_size\"] = self.config[\"hidden_size\"] + self.config[\"ticker_embedd_dim\"]\n",
    "        if self.config[\"ticker_embedd_dim\"] != 0:\n",
    "            self.ticker_embedd = nn.Embedding(self.config[\"tick_count\"], self.config[\"ticker_embedd_dim\"])\n",
    "        self.block_ts = BLOCK_TS(self.config[\"block_ts_config\"])\n",
    "        # self.news_proj = nn.Sequential(\n",
    "        #     nn.Linear(self.config[\"news_embedd\"], self.config[\"hidden_size\"]),\n",
    "        #     nn.LayerNorm(self.config[\"hidden_size\"]),\n",
    "        #     nn.SiLU()\n",
    "        # )\n",
    "        self.news_proj = nn.Sequential(\n",
    "            nn.Linear(self.config[\"news_embedd\"], self.config[\"hidden_size\"]),\n",
    "            nn.LayerNorm(self.config[\"hidden_size\"])\n",
    "        )\n",
    "        self.cross_attention = nn.ModuleList(CrossAttention(self.config[\"hidden_size\"], self.config[\"num_heads\"]) for _ in range(self.config[\"attention_layer\"]))\n",
    "        self.predict_fc = nn.Sequential(\n",
    "                nn.LayerNorm(self.config[\"hidden_size\"] + self.config[\"n_add_feature_ts\"]),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(self.config[\"hidden_size\"] + self.config[\"n_add_feature_ts\"], 1)\n",
    "        ) \n",
    "    def forward(self, time:torch.Tensor, add_time:torch.Tensor, news:torch.Tensor, news_mask:torch.Tensor=None, ticker:torch.Tensor=None, return_logit:bool=False):\n",
    "        # FEATURE\n",
    "        time = self.block_ts(time)\n",
    "        if self.config[\"ticker_embedd_dim\"] != 0:\n",
    "            ticker_embedd = self.ticker_embedd(ticker)\n",
    "            time = torch.cat([time, ticker_embedd], dim=1)\n",
    "        news = self.news_proj(news)\n",
    "        if news_mask is not None:\n",
    "            news = news * news_mask.unsqueeze(-1)\n",
    "        # attention\n",
    "        time = time.unsqueeze(1)\n",
    "        for layer in self.cross_attention:\n",
    "            time = time + layer(time, news)\n",
    "        time = time.squeeze(1)\n",
    "        time = torch.cat([time, add_time], dim=1)\n",
    "        # FC\n",
    "        if return_logit:\n",
    "            predict_price = self.predict_fc(time)\n",
    "        else:\n",
    "            predict_price = nn.functional.sigmoid(self.predict_fc(time))\n",
    "        return predict_price.reshape([-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_experiment = \"TRANSFORMER_classs\"\n",
    "# mlflow.set_experiment(name_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "            \"attention_layer\" : 4,\n",
    "            \"hidden_size\" : 64,\n",
    "            \"num_heads\" : 4,\n",
    "            \"news_embedd\" : 768,\n",
    "            \"n_add_feature_ts\" : 5,\n",
    "            \"tick_count\" : len(tickers_),\n",
    "            \"ticker_embedd_dim\":4,\n",
    "            \"block_ts_config\" : {\n",
    "                \"x_feature\" : X_time_train.shape[-1],\n",
    "                \"hidden_size\" : 36,\n",
    "                \"num_layers\" : 4,\n",
    "            },\n",
    "            \"tokenizer\":tokenizer.name_or_path\n",
    "        }\n",
    "\n",
    "scheduler = {\n",
    "    \"step_size\":300,\n",
    "    \"gamma\":0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 61: 100%|██████████| 121/121 [00:01<00:00, 64.95it/s]\n",
      "Epoch: 61: 100%|██████████| 16/16 [00:00<00:00, 130.08it/s]\n",
      "Epoch: 62: 100%|██████████| 121/121 [00:01<00:00, 66.79it/s]\n",
      "Epoch: 62: 100%|██████████| 16/16 [00:00<00:00, 108.12it/s]\n",
      "Epoch: 63: 100%|██████████| 121/121 [00:01<00:00, 62.19it/s]\n",
      "Epoch: 63: 100%|██████████| 16/16 [00:00<00:00, 111.88it/s]\n",
      "Epoch: 64: 100%|██████████| 121/121 [00:01<00:00, 60.68it/s]\n",
      "Epoch: 64: 100%|██████████| 16/16 [00:00<00:00, 93.02it/s]\n",
      "Epoch: 65: 100%|██████████| 121/121 [00:02<00:00, 58.71it/s]\n",
      "Epoch: 65: 100%|██████████| 16/16 [00:00<00:00, 137.93it/s]\n",
      "Epoch: 66: 100%|██████████| 121/121 [00:01<00:00, 62.14it/s]\n",
      "Epoch: 66: 100%|██████████| 16/16 [00:00<00:00, 119.39it/s]\n",
      "Epoch: 67: 100%|██████████| 121/121 [00:01<00:00, 70.12it/s]\n",
      "Epoch: 67: 100%|██████████| 16/16 [00:00<00:00, 111.89it/s]\n",
      "Epoch: 68: 100%|██████████| 121/121 [00:01<00:00, 72.81it/s]\n",
      "Epoch: 68: 100%|██████████| 16/16 [00:00<00:00, 146.79it/s]\n",
      "Epoch: 69:  39%|███▉      | 47/121 [00:00<00:00, 80.18it/s]\n",
      "  5%|▍         | 69/1500 [03:37<1:15:12,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save with error\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Первый запуск\n",
    "try:\n",
    "    run = mlflow.start_run()\n",
    "    shutil.rmtree(\"artifacts/\", ignore_errors=True)\n",
    "    os.makedirs(\"artifacts/\", exist_ok=True)\n",
    "    with open(\"artifacts/model_config.json\", \"w\") as f:\n",
    "        json.dump(model_config, f)\n",
    "    model, optimizer = train_model(model_config, device, TRAIN_LOADER, TEST_LOADER,\n",
    "                                   epochs=1500, lr=1e-5, weight_decay=0.1)\n",
    "finally:\n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
